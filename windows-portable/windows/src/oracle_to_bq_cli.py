#!/usr/bin/env python3
"""
Oracle to BigQuery Migration Tool - Simplified CLI for Portable Version
pandas ì˜ì¡´ì„± ì—†ì´ ì‘ë™í•˜ëŠ” ê°„ë‹¨í•œ ë²„ì „
"""

import sys
import csv
import json
import argparse
from pathlib import Path
from typing import Dict, List, Any, Optional

class SimpleMigrationTool:
    """ê°„ë‹¨í•œ ë§ˆì´ê·¸ë ˆì´ì…˜ ë„êµ¬ (pandas ì—†ìŒ)"""
    
    def __init__(self, config_file=None):
        """
        Args:
            config_file: ì„¤ì • íŒŒì¼ ê²½ë¡œ
        """
        # ê¸°ë³¸ ì„¤ì •ê°’
        self.project_id = 'your_project'
        self.string_mode = 'auto'
        self.preserve_string_length = False
        self.use_schema_as_dataset = True  # Oracle ìŠ¤í‚¤ë§ˆëª…ì„ ë°ì´í„°ì…‹ëª…ìœ¼ë¡œ ì‚¬ìš©
        self.merge_output = True  # ëª¨ë“  DDLì„ í•˜ë‚˜ì˜ íŒŒì¼ë¡œ ë³‘í•© (ê¸°ë³¸ê°’)
        self.create_primary_keys = True  # ê¸°ë³¸í‚¤ ì œì•½ì¡°ê±´ ìƒì„±
        self.create_or_replace = False  # CREATE OR REPLACE TABLE ì‚¬ìš© ì—¬ë¶€
        self.enable_partitioning = True  # íŒŒí‹°ì…”ë‹ ê¸°ëŠ¥ í™œì„±í™”
        self.enable_clustering = True  # í´ëŸ¬ìŠ¤í„°ë§ ê¸°ëŠ¥ í™œì„±í™”
        self.partition_expiration_days = None  # íŒŒí‹°ì…˜ ë§Œë£Œ ì¼ìˆ˜
        self.debug_mode = False  # ë””ë²„ê·¸ ì¶œë ¥ í™œì„±í™”
        self.drop_partition_table_before_create = False  # íŒŒí‹°ì…˜ í…Œì´ë¸” ìƒì„± ì „ DROP ì‹¤í–‰
        self.output_filename = 'merged_ddl.sql'  # ë³‘í•© íŒŒì¼ëª… (ê¸°ë³¸ê°’)
        
        # ì„¤ì • íŒŒì¼ ë¡œë“œ
        self.load_config(config_file)
        
        self.type_mappings = {
            'VARCHAR2': 'STRING',
            'CHAR': 'STRING', 
            'NVARCHAR2': 'STRING',
            'NCHAR': 'STRING',
            'NUMBER': 'INT64',
            'INTEGER': 'INT64',
            'FLOAT': 'FLOAT64',
            'DATE': 'DATE',
            'TIMESTAMP': 'TIMESTAMP',
            'CLOB': 'STRING',
            'BLOB': 'BYTES',
            'RAW': 'BYTES'
        }
    
    def load_config(self, config_file=None):
        """ì„¤ì • íŒŒì¼ ë¡œë“œ (JSON í˜•ì‹)"""
        config_paths = []
        
        if config_file:
            config_paths.append(Path(config_file))
        else:
            # --config ì˜µì…˜ì´ ì—†ì„ ë•Œ ì‹¤í–‰íŒŒì¼ê³¼ ë™ì¼í•œ ê²½ë¡œì˜ config.jsonì„ ìš°ì„  ì°¸ì¡°
            script_dir = Path(__file__).parent.parent  # srcì˜ ìƒìœ„ ë””ë ‰í† ë¦¬ (windows ë””ë ‰í† ë¦¬)
            default_config = script_dir / 'config.json'
            config_paths.append(default_config)
        
        # ê¸°ë³¸ ì„¤ì • íŒŒì¼ ê²½ë¡œë“¤ (ì‹¤í–‰íŒŒì¼ ê²½ë¡œ config.jsonì´ ì—†ì„ ë•Œ ëŒ€ì²´)
        config_paths.extend([
            Path('oracle_to_bq_config.json'),
            Path('config.json'),
            Path('config/oracle_to_bq.json'),
            Path('config/config.json')
        ])
        
        for config_path in config_paths:
            if config_path.exists():
                try:
                    with open(config_path, 'r', encoding='utf-8') as f:
                        config = json.load(f)
                    
                    if config:
                        # ì„¤ì •ê°’ ì ìš© - project_idê°€ configì— ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ë¡œ ì„¤ì •
                        if 'project_id' in config:
                            self.project_id = config['project_id']
                        else:
                            self.project_id = ''  # configì— project_idê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´
                        
                        self.string_mode = config.get('string_mode', self.string_mode)
                        self.preserve_string_length = config.get('preserve_string_length', self.preserve_string_length)
                        self.use_schema_as_dataset = config.get('use_schema_as_dataset', self.use_schema_as_dataset)
                        self.create_or_replace = config.get('create_or_replace', self.create_or_replace)
                        self.enable_partitioning = config.get('enable_partitioning', self.enable_partitioning)
                        self.enable_clustering = config.get('enable_clustering', self.enable_clustering)
                        self.partition_expiration_days = config.get('partition_expiration_days', self.partition_expiration_days)
                        self.debug_mode = config.get('debug_mode', self.debug_mode)
                        self.drop_partition_table_before_create = config.get('drop_partition_table_before_create', self.drop_partition_table_before_create)
                        
                        print(f"âœ“ ì„¤ì • íŒŒì¼ ë¡œë“œë¨: {config_path}")
                        return
                        
                except Exception as e:
                    print(f"âš ï¸ ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({config_path}): {e}")
                    continue
    
    def create_default_config(self, config_path='oracle_to_bq_config.json'):
        """ê¸°ë³¸ ì„¤ì • íŒŒì¼ ìƒì„±"""
        default_config = {
            "project_id": "your_project",
            "string_mode": "auto",
            "preserve_string_length": False,
            "use_schema_as_dataset": True,
            "create_or_replace": False,
            "_comments": {
                "project_id": "BigQuery í”„ë¡œì íŠ¸ ID",
                "string_mode": "ë¬¸ìì—´ ë³€í™˜ ëª¨ë“œ: 'auto' ë˜ëŠ” 'string_only'",
                "preserve_string_length": "STRING íƒ€ì…ì— ê¸¸ì´ ì •ë³´ í¬í•¨ ì—¬ë¶€ (ì˜ˆ: STRING(100))",
                "use_schema_as_dataset": "Oracle ìŠ¤í‚¤ë§ˆëª…ì„ ë°ì´í„°ì…‹ëª…ìœ¼ë¡œ ì‚¬ìš© ì—¬ë¶€",
                "create_or_replace": "CREATE OR REPLACE TABLE ì‚¬ìš© ì—¬ë¶€ (true: CREATE OR REPLACE, false: CREATE)"
            }
        }
        
        try:
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(default_config, f, indent=2, ensure_ascii=False)
            print(f"âœ“ ê¸°ë³¸ ì„¤ì • íŒŒì¼ ìƒì„±ë¨: {config_path}")
            return True
        except Exception as e:
            print(f"âŒ ì„¤ì • íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {e}")
            return False
    

    
    def create_config_template(self, output_file: str):
        """ì„¤ì • íŒŒì¼ í…œí”Œë¦¿ ìƒì„±"""
        config_template = {
            "project_id": "your-bigquery-project-id",
            "string_mode": "auto",
            "preserve_string_length": False,
            "description": {
                "project_id": "BigQuery í”„ë¡œì íŠ¸ ID",
                "string_mode": "ë¬¸ìì—´ ë³€í™˜ ëª¨ë“œ (auto ë˜ëŠ” string_only)",
                "preserve_string_length": "STRING íƒ€ì…ì— ê¸¸ì´ ì •ë³´ í¬í•¨ ì—¬ë¶€"
            }
        }
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(config_template, f, indent=2, ensure_ascii=False)
            print(f"âœ“ ì„¤ì • íŒŒì¼ í…œí”Œë¦¿ ìƒì„±ë¨: {output_file}")
            print("  íŒŒì¼ì„ í¸ì§‘í•˜ì—¬ í”„ë¡œì íŠ¸ IDì™€ ì˜µì…˜ì„ ì„¤ì •í•˜ì„¸ìš”.")
        except Exception as e:
            print(f"âŒ ì„¤ì • íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {e}")
    
    def convert_oracle_type(self, oracle_type: str, precision: Optional[str] = None, scale: Optional[str] = None) -> str:
        """Oracle íƒ€ì…ì„ BigQuery íƒ€ì…ìœ¼ë¡œ ë³€í™˜ (ì •ë°€ë„ì™€ ìŠ¤ì¼€ì¼ ì •ë³´ ë³´ì¡´)"""
        base_type = oracle_type.upper().split('(')[0]
        
        # string_only ëª¨ë“œëŠ” ë¬¸ìì—´ íƒ€ì…ì—ë§Œ ì˜í–¥ì„ ì¤Œ (ë‹¤ë¥¸ íƒ€ì…ì€ ì •ìƒ ë³€í™˜)
        
        # auto ëª¨ë“œ: Oracle íƒ€ì…ì— ë”°ë¼ ìµœì ì˜ BigQuery íƒ€ì…ìœ¼ë¡œ ë³€í™˜
        if base_type == 'NUMBER':
            # NUMBER íƒ€ì…ì˜ ì •ë°€í•œ ë³€í™˜ ë¡œì§
            if precision is None and scale is None:
                # NUMBER without precision/scale -> NUMERIC (ì •ë°€ë„ ë³´ì¡´)
                return 'NUMERIC'
            
            # ì •ë°€ë„ì™€ ìŠ¤ì¼€ì¼ì„ ìˆ«ìë¡œ ë³€í™˜
            try:
                prec = int(precision) if precision and str(precision).strip() else None
                sc = int(scale) if scale and str(scale).strip() else None
            except (ValueError, TypeError):
                prec = None
                sc = None
            
            # NUMBER with scale 0 (ì •ìˆ˜í˜•)
            if sc is not None and sc == 0:
                if prec is not None and prec <= 18:
                    return 'INT64'  # INT64 ë²”ìœ„ ë‚´ì˜ ì •ìˆ˜
                elif prec is not None and prec <= 29:
                    return 'NUMERIC'  # NUMERIC(P, 0)ì—ì„œ P <= 29
                else:
                    return 'BIGNUMERIC'  # í° ì •ìˆ˜ëŠ” BIGNUMERICìœ¼ë¡œ ì²˜ë¦¬
            
            # NUMBER with scale > 0 (ì†Œìˆ˜ì  í¬í•¨)
            if sc is not None and sc > 0:
                if prec is not None:
                    # BigQuery NUMERIC í•œê³„ í™•ì¸ (38ìë¦¬ ì •ë°€ë„, 9ìë¦¬ ì†Œìˆ˜ì )
                    if prec <= 38 and sc <= 9:
                        return 'NUMERIC'
                    # BigQuery NUMERIC í•œê³„ë¥¼ ì´ˆê³¼í•˜ëŠ” ê²½ìš° BIGNUMERIC ì‚¬ìš©
                    elif prec <= 76 and sc <= 38:
                        return 'BIGNUMERIC'
                    else:
                        # ê·¹í•œì˜ ì •ë°€ë„ëŠ” STRINGìœ¼ë¡œ ì²˜ë¦¬
                        return 'STRING'
                else:
                    return 'NUMERIC'
            
            # NUMBER with negative scale (ì†Œìˆ˜ì  ì™¼ìª½ ë°˜ì˜¬ë¦¼)
            if sc is not None and sc < 0:
                return 'NUMERIC'
            
            # ê¸°íƒ€ ëª¨ë“  ê²½ìš° NUMERICìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
            return 'NUMERIC'
        
        # TIMESTAMP íƒ€ì…ë“¤
        if base_type.startswith('TIMESTAMP'):
            return 'DATETIME'
        
        # ë¬¸ìì—´ íƒ€ì…ë“¤
        if base_type in ['VARCHAR2', 'CHAR', 'NVARCHAR2', 'NCHAR', 'CLOB', 'NCLOB', 'LONG']:
            return 'STRING'
        
        # ë°”ì´ë„ˆë¦¬ íƒ€ì…ë“¤
        if base_type in ['BLOB', 'RAW']:
            return 'BYTES'
        
        # DATE íƒ€ì…
        if base_type == 'DATE':
            return 'DATETIME'
        
        # ê¸°íƒ€ íƒ€ì…ë“¤
        return self.type_mappings.get(base_type, 'STRING')
    
    def detect_encoding(self, file_path: Path) -> str:
        """íŒŒì¼ ì¸ì½”ë”©ì„ ìë™ ê°ì§€ (UTF-8, EUC-KR ì§€ì›)"""
        encodings = ['utf-8', 'euc-kr', 'cp949']
        
        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding) as f:
                    # íŒŒì¼ì˜ ì²« ëª‡ ì¤„ì„ ì½ì–´ì„œ ì¸ì½”ë”©ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸
                    f.read(1024)
                    print(f"âœ“ íŒŒì¼ ì¸ì½”ë”© ê°ì§€: {encoding}")
                    return encoding
            except UnicodeDecodeError:
                continue
            except Exception:
                continue
        
        # ê¸°ë³¸ê°’ìœ¼ë¡œ UTF-8 ë°˜í™˜
        print("âš ï¸ ì¸ì½”ë”© ê°ì§€ ì‹¤íŒ¨, UTF-8ë¡œ ì‹œë„í•©ë‹ˆë‹¤.")
        return 'utf-8'
    
    def process_csv_file(self, input_file: Path, output_dir: Path) -> bool:
        """CSV íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ BigQuery DDL ìƒì„±"""
        try:
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # íŒŒì¼ ì¸ì½”ë”© ìë™ ê°ì§€
            encoding = self.detect_encoding(input_file)
            
            with open(input_file, 'r', encoding=encoding) as f:
                reader = csv.DictReader(f)
                
                # í…Œì´ë¸”ë³„ë¡œ ê·¸ë£¹í™” (ìŠ¤í‚¤ë§ˆëª… í¬í•¨)
                tables = {}
                schemas = set()
                
                for row in reader:
                    table_name = row.get('TABLE_NAME', '')
                    # Oracle ìŠ¤í‚¤ë§ˆëª…ì„ BigQuery ë°ì´í„°ì…‹ëª…ìœ¼ë¡œ ì‚¬ìš©
                    # ìš°ì„ ìˆœìœ„: OWNER > SCHEMA_NAME > TABLE_SCHEMA
                    schema_name = row.get('OWNER', '') or row.get('SCHEMA_NAME', '') or row.get('TABLE_SCHEMA', '')
                    
                    if not table_name:
                        continue
                    
                    # Oracle ìŠ¤í‚¤ë§ˆëª…ì´ ìˆìœ¼ë©´ ìˆ˜ì§‘ (BigQuery ë°ì´í„°ì…‹ëª…ìœ¼ë¡œ ì‚¬ìš©ë¨)
                    if schema_name:
                        schemas.add(schema_name)
                    
                    # í…Œì´ë¸” í‚¤ ìƒì„± (ìŠ¤í‚¤ë§ˆëª… í¬í•¨ ê°€ëŠ¥)
                    table_key = table_name
                    if schema_name:
                        table_key = f"{schema_name}.{table_name}"
                    
                    if table_key not in tables:
                        tables[table_key] = {
                            'schema_name': schema_name if schema_name else None,
                            'table_name': table_name,
                            'columns': []
                        }
                    
                    column_info = {
                        'column_name': row.get('COLUMN_NAME', ''),
                        'data_type': row.get('DATA_TYPE', ''),
                        'data_precision': row.get('DATA_PRECISION', ''),
                        'data_scale': row.get('DATA_SCALE', ''),
                        'char_length': row.get('CHAR_LENGTH', '') or row.get('DATA_LENGTH', ''),
                        'data_length': row.get('DATA_LENGTH', ''),
                        'nullable': row.get('NULLABLE', 'Y'),
                        'is_primary_key': row.get('IS_PRIMARY_KEY', 'N'),
                        'fk_constraint_name': row.get('FK_CONSTRAINT_NAME', ''),
                        'unique_constraint_name': row.get('UNIQUE_CONSTRAINT_NAME', '') or row.get('UK_CONSTRAINT_NAME', ''),
                        'default_value': row.get('DEFAULT_VALUE', '') or row.get('DATA_DEFAULT', ''),
                        'data_default': row.get('DATA_DEFAULT', ''),
                        'column_comment': row.get('COLUMN_COMMENT', '') or row.get('COMMENTS', ''),
                        # íŒŒí‹°ì…”ë‹ê³¼ í´ëŸ¬ìŠ¤í„°ë§ ê´€ë ¨ ì»¬ëŸ¼ë“¤ ì¶”ê°€ (ê°„ì†Œí™”)
                        'partition_yn': row.get('PARTITION_YN', 'N'),
                        'cluster_yn': row.get('CLUSTER_YN', 'N')
                    }
                    tables[table_key]['columns'].append(column_info)
            
            # ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶œë ¥
            if schemas:
                print(f"âœ“ ë°œê²¬ëœ ìŠ¤í‚¤ë§ˆ: {', '.join(sorted(schemas))}")
            
            # DDL ìƒì„± ë°©ì‹ ê²°ì • (ê°œë³„ íŒŒì¼ vs ë³‘í•© íŒŒì¼)
            if self.merge_output:
                # ëª¨ë“  DDLì„ í•˜ë‚˜ì˜ íŒŒì¼ë¡œ ë³‘í•©
                merged_file = output_dir / self.output_filename
                self.generate_merged_ddl(tables, merged_file)
                print(f"âœ“ {len(tables)}ê°œ í…Œì´ë¸” DDLì„ ë³‘í•© íŒŒì¼ë¡œ ìƒì„± ì™„ë£Œ: {merged_file}")
            else:
                # ê° í…Œì´ë¸”ì— ëŒ€í•´ ê°œë³„ DDL íŒŒì¼ ìƒì„±
                for table_key, table_info in tables.items():
                    schema_name = table_info['schema_name']
                    table_name = table_info['table_name']
                    columns = table_info['columns']
                    
                    # íŒŒì¼ëª… ìƒì„± (ìŠ¤í‚¤ë§ˆëª… í¬í•¨)
                    if schema_name:
                        ddl_file = output_dir / f"{schema_name}_{table_name}.sql"
                    else:
                        ddl_file = output_dir / f"{table_name}.sql"
                    
                    self.generate_ddl(schema_name, table_name, columns, ddl_file)
                
                print(f"âœ“ {len(tables)}ê°œ í…Œì´ë¸” DDL ìƒì„± ì™„ë£Œ: {output_dir}")
            
            return True
            
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return False
    
    def needs_backticks(self, name: str) -> bool:
        """ì´ë¦„ì— ë°±í‹±ì´ í•„ìš”í•œì§€ í™•ì¸ (í•œê¸€, íŠ¹ìˆ˜ë¬¸ì, ì˜ˆì•½ì–´ ë“±)"""
        import re
        
        # í•œê¸€ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        if re.search(r'[ê°€-í£]', name):
            return True
        
        # ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ”ì§€ í™•ì¸
        if name and name[0].isdigit():
            return True
        
        # íŠ¹ìˆ˜ë¬¸ìê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ (ì–¸ë”ìŠ¤ì½”ì–´ ì œì™¸)
        if re.search(r'[^a-zA-Z0-9_]', name):
            return True
        
        # BigQuery ì˜ˆì•½ì–´ í™•ì¸ (ì¼ë¶€ë§Œ)
        reserved_words = {
            'ALL', 'AND', 'ANY', 'ARRAY', 'AS', 'ASC', 'ASSERT_ROWS_MODIFIED',
            'AT', 'BETWEEN', 'BY', 'CASE', 'CAST', 'COLLATE', 'CONTAINS',
            'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'DEFAULT', 'DEFINE',
            'DESC', 'DISTINCT', 'ELSE', 'END', 'ENUM', 'ESCAPE', 'EXCEPT',
            'EXCLUDE', 'EXISTS', 'EXTRACT', 'FALSE', 'FETCH', 'FOLLOWING',
            'FOR', 'FROM', 'FULL', 'GROUP', 'GROUPING', 'GROUPS', 'HASH',
            'HAVING', 'IF', 'IGNORE', 'IN', 'INNER', 'INTERSECT', 'INTERVAL',
            'INTO', 'IS', 'JOIN', 'LATERAL', 'LEFT', 'LIKE', 'LIMIT',
            'LOOKUP', 'MERGE', 'NATURAL', 'NEW', 'NO', 'NOT', 'NULL',
            'NULLS', 'OF', 'ON', 'OR', 'ORDER', 'OUTER', 'OVER',
            'PARTITION', 'PRECEDING', 'PROTO', 'RANGE', 'RECURSIVE',
            'RESPECT', 'RIGHT', 'ROLLUP', 'ROWS', 'SELECT', 'SET',
            'SOME', 'STRUCT', 'TABLESAMPLE', 'THEN', 'TO', 'TREAT',
            'TRUE', 'UNBOUNDED', 'UNION', 'UNNEST', 'USING', 'WHEN',
            'WHERE', 'WINDOW', 'WITH', 'WITHIN'
        }
        
        if name.upper() in reserved_words:
            return True
        
        return False
    
    def format_identifier(self, name: str) -> str:
        """ì‹ë³„ìë¥¼ ì ì ˆíˆ í¬ë§·íŒ… (í•„ìš”ì‹œ ë°±í‹± ì¶”ê°€)"""
        if self.needs_backticks(name):
            return f"`{name}`"
        return name
    
    def generate_ddl(self, schema_name: Optional[str], table_name: str, columns: List[Dict], output_file: Path):
        """BigQuery DDL ìƒì„± (ì •ë°€ë„, ìŠ¤ì¼€ì¼, ê¸¸ì´, ì„¤ëª… ì •ë³´ í¬í•¨)"""
        ddl_content = self.create_table_ddl(schema_name, table_name, columns)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(ddl_content)
    
    def generate_merged_ddl(self, tables: Dict, output_file: Path):
        """ëª¨ë“  í…Œì´ë¸”ì˜ DDLì„ í•˜ë‚˜ì˜ íŒŒì¼ë¡œ ë³‘í•© ìƒì„±"""
        ddl_sections = []
        
        # íŒŒì¼ í—¤ë”
        ddl_sections.append("-- Oracle to BigQuery DDL Migration")
        ddl_sections.append(f"-- Generated on: {self.get_current_timestamp()}")
        ddl_sections.append(f"-- Total tables: {len(tables)}")
        ddl_sections.append("")
        
        for table_key, table_info in tables.items():
            schema_name = table_info['schema_name']
            table_name = table_info['table_name']
            columns = table_info['columns']
            
            # í…Œì´ë¸”ë³„ ì„¹ì…˜ êµ¬ë¶„
            ddl_sections.append(f"-- ========================================")
            ddl_sections.append(f"-- Table: {table_name}")
            if schema_name:
                ddl_sections.append(f"-- Schema: {schema_name}")
            ddl_sections.append(f"-- ========================================")
            ddl_sections.append("")
            
            # í…Œì´ë¸” DDL ìƒì„±
            table_ddl = self.create_table_ddl(schema_name, table_name, columns)
            ddl_sections.append(table_ddl)
            ddl_sections.append("")
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("\n".join(ddl_sections))
    
    def create_table_ddl(self, schema_name: Optional[str], table_name: str, columns: List[Dict]) -> str:
        """ê°œë³„ í…Œì´ë¸”ì˜ DDL ë¬¸ìì—´ ìƒì„±"""
        # BigQuery ë°ì´í„°ì…‹ëª… ê²°ì • (Oracle OWNER/ìŠ¤í‚¤ë§ˆëª…ì˜ ì›ë³¸ ëŒ€ì†Œë¬¸ì ìœ ì§€)
        dataset_name = schema_name if schema_name else 'your_dataset'
        
        # í…Œì´ë¸”ëª… í¬ë§·íŒ… (ë°±í‹± ì—†ì´ ì •ë¦¬, ì›ë³¸ ëŒ€ì†Œë¬¸ì ìœ ì§€)
        clean_table_name = table_name.strip('`')
        
        # ì „ì²´ í…Œì´ë¸”ëª… ìƒì„± - project_idê°€ ì—†ìœ¼ë©´ ë°ì´í„°ì…‹.í…Œì´ë¸”ëª… í˜•íƒœë¡œ
        if self.project_id and self.project_id.strip():
            full_table_name = f"`{self.project_id}.{dataset_name}.{clean_table_name}`"
        else:
            full_table_name = f"`{dataset_name}.{clean_table_name}`"
        
        # íŒŒí‹°ì…˜ í…Œì´ë¸” ì—¬ë¶€ í™•ì¸ (ë‚˜ì¤‘ì— ì‚¬ìš©)
        has_partition = any(col.get('partition_yn', 'N').upper() == 'Y' for col in columns)
        
        # DROP ë¬¸ ì¶”ê°€ (íŒŒí‹°ì…˜ í…Œì´ë¸”ì´ê³  ì˜µì…˜ì´ í™œì„±í™”ëœ ê²½ìš°)
        ddl_lines = []
        if self.create_or_replace and has_partition and self.drop_partition_table_before_create:
            ddl_lines.append(f"DROP TABLE IF EXISTS {full_table_name};")
            ddl_lines.append("")  # ë¹ˆ ì¤„ ì¶”ê°€
        
        # CREATE ë˜ëŠ” CREATE OR REPLACE ì„ íƒ
        create_statement = "CREATE OR REPLACE TABLE" if self.create_or_replace else "CREATE TABLE"
        ddl_lines.append(f"{create_statement} {full_table_name} (")
        
        column_definitions = []
        primary_key_columns = []
        
        for col in columns:
            col_name = col['column_name']
            oracle_type = col['data_type']
            precision = col['data_precision']
            scale = col['data_scale']
            char_length = col.get('char_length') or col.get('data_length')
            nullable = col['nullable']
            is_primary_key = col.get('is_primary_key', 'N').upper()
            
            bq_type = self.convert_oracle_type(oracle_type, precision, scale)
            
            # ì»¬ëŸ¼ëª… í¬ë§·íŒ… (ë°±í‹± ì²˜ë¦¬)
            formatted_col_name = self.format_identifier(col_name)
            
            # BigQuery íƒ€ì…ì— ì •ë°€ë„/ìŠ¤ì¼€ì¼ ì •ë³´ ì¶”ê°€
            type_with_precision = self.format_bigquery_type_with_precision(
                bq_type, oracle_type, precision, scale, char_length
            )
            
            # ì»¬ëŸ¼ ì •ì˜
            col_def = f"  {formatted_col_name} {type_with_precision}"
            if nullable == 'N':
                col_def += " NOT NULL"
            
            # ì„¤ëª… ì¶”ê°€ (Oracle íƒ€ì… ì •ë³´ í¬í•¨)
            description = self.create_column_description(col)
            if description:
                # BigQueryì—ì„œëŠ” OPTIONSë¡œ description ì¶”ê°€
                col_def += f" OPTIONS(description=\"{self.escape_description(description)}\")"
            
            column_definitions.append(col_def)
            
            # ê¸°ë³¸í‚¤ ì»¬ëŸ¼ ìˆ˜ì§‘
            if self.create_primary_keys and is_primary_key == 'Y':
                primary_key_columns.append(formatted_col_name)
        
        ddl_lines.append(",\n".join(column_definitions))
        
        # ê¸°ë³¸í‚¤ ì œì•½ì¡°ê±´ ì¶”ê°€ (BigQueryëŠ” PRIMARY KEYë¥¼ ì§€ì›í•˜ì§€ë§Œ enforcedë˜ì§€ ì•ŠìŒ)
        # BigQueryëŠ” ìµœëŒ€ 16ê°œì˜ ê¸°ë³¸í‚¤ ì»¬ëŸ¼ë§Œ ì§€ì›
        if primary_key_columns:
            if len(primary_key_columns) > 16:
                if self.debug_mode:
                    print(f"WARNING: ê¸°ë³¸í‚¤ ì»¬ëŸ¼ì´ {len(primary_key_columns)}ê°œì…ë‹ˆë‹¤. BigQueryëŠ” ìµœëŒ€ 16ê°œë§Œ ì§€ì›í•˜ë¯€ë¡œ ì²˜ìŒ 16ê°œë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                primary_key_columns = primary_key_columns[:16]
            
            ddl_lines.append(",")
            pk_constraint = f"  PRIMARY KEY ({', '.join(primary_key_columns)}) NOT ENFORCED"
            ddl_lines.append(pk_constraint)
        
        ddl_lines.append(")")
        
        # íŒŒí‹°ì…”ë‹ê³¼ í´ëŸ¬ìŠ¤í„°ë§ ì¶”ê°€
        partition_clause, cluster_clause = self.generate_partition_cluster_clauses(columns)
        
        if partition_clause:
            ddl_lines.append(partition_clause)
        
        if cluster_clause:
            ddl_lines.append(cluster_clause)
        
        # íŒŒí‹°ì…˜ ë§Œë£Œ ì„¤ì • ì¶”ê°€
        if self.enable_partitioning and self.partition_expiration_days:
            expiration_clause = f"OPTIONS(partition_expiration_days={self.partition_expiration_days})"
            ddl_lines.append(expiration_clause)
        
        ddl_lines.append(";")
        
        return "\n".join(ddl_lines)
    
    def get_current_timestamp(self) -> str:
        """í˜„ì¬ íƒ€ì„ìŠ¤íƒ¬í”„ ë°˜í™˜"""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    def generate_partition_cluster_clauses(self, columns: List[Dict]) -> tuple:
        """íŒŒí‹°ì…”ë‹ê³¼ í´ëŸ¬ìŠ¤í„°ë§ ì ˆ ìƒì„±"""
        partition_clause = None
        cluster_clause = None
        
        if not self.enable_partitioning and not self.enable_clustering:
            return partition_clause, cluster_clause
        
        # íŒŒí‹°ì…”ë‹ ì„¤ì • í™•ì¸
        partition_columns = []
        partition_type = None
        
        # í´ëŸ¬ìŠ¤í„°ë§ ì„¤ì • í™•ì¸
        cluster_columns = []
        
        for col in columns:
            column_name = col.get('column_name', '')
            if self.debug_mode:
                print(f"DEBUG: ì»¬ëŸ¼ {column_name} - partition_yn: {col.get('partition_yn', 'N')}, cluster_yn: {col.get('cluster_yn', 'N')}")
            
            # íŒŒí‹°ì…”ë‹ í™•ì¸ (PARTITION_YN = 'Y')
            partition_yn = col.get('partition_yn', 'N')
            if (self.enable_partitioning and 
                partition_yn and str(partition_yn).upper() == 'Y'):
                data_type = col.get('data_type', '').upper()
                if column_name:
                    if self.debug_mode:
                        print(f"DEBUG: íŒŒí‹°ì…˜ ì»¬ëŸ¼ ì¶”ê°€: {column_name} ({data_type})")
                    partition_columns.append((column_name, data_type))
            
            # í´ëŸ¬ìŠ¤í„°ë§ í™•ì¸ (CLUSTER_YN = 'Y')
            cluster_yn = col.get('cluster_yn', 'N')
            if (self.enable_clustering and 
                cluster_yn and str(cluster_yn).upper() == 'Y'):
                if column_name:
                    if self.debug_mode:
                        print(f"DEBUG: í´ëŸ¬ìŠ¤í„° ì»¬ëŸ¼ ì¶”ê°€: {column_name}")
                    cluster_columns.append(column_name)
        
        # íŒŒí‹°ì…˜ ì ˆ ìƒì„± (ì²« ë²ˆì§¸ íŒŒí‹°ì…˜ ì»¬ëŸ¼ë§Œ ì‚¬ìš©)
        if partition_columns:
            partition_col, oracle_data_type = partition_columns[0]
            
            # Oracle íƒ€ì…ì„ BigQuery íƒ€ì…ìœ¼ë¡œ ë³€í™˜
            precision = None
            scale = None
            for col in columns:
                if col.get('column_name') == partition_col:
                    precision = col.get('data_precision')
                    scale = col.get('data_scale')
                    break
            
            bq_type = self.convert_oracle_type(oracle_data_type, precision, scale)
            
            # BigQueryì—ì„œ íŒŒí‹°ì…˜ì„ ì§€ì›í•˜ëŠ” íƒ€ì…ë§Œ ì²˜ë¦¬
            # ì§€ì› íƒ€ì…: DATE, TIMESTAMP, DATETIME, INT64 (RANGE íŒŒí‹°ì…˜ìš©)
            formatted_col = self.format_identifier(partition_col)
            
            if bq_type == 'DATE':
                partition_clause = f"PARTITION BY DATE({formatted_col})"
            elif bq_type == 'TIMESTAMP':
                partition_clause = f"PARTITION BY DATE({formatted_col})"
            elif bq_type == 'DATETIME':
                partition_clause = f"PARTITION BY DATETIME_TRUNC({formatted_col}, DAY)"
            elif bq_type in ['INT64', 'NUMERIC', 'BIGNUMERIC']:
                # INTEGER RANGE íŒŒí‹°ì…˜ (ì„ íƒì )
                # ê¸°ë³¸ì ìœ¼ë¡œëŠ” ìƒì„±í•˜ì§€ ì•ŠìŒ (ë²”ìœ„ ì„¤ì •ì´ í•„ìš”í•˜ë¯€ë¡œ)
                if self.debug_mode:
                    print(f"WARNING: ìˆ«ì íƒ€ì…({bq_type}) íŒŒí‹°ì…˜ì€ RANGE íŒŒí‹°ì…˜ ì„¤ì •ì´ í•„ìš”í•˜ì—¬ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                partition_clause = None
            else:
                # ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒ€ì… (STRING, BYTES ë“±)
                if self.debug_mode:
                    print(f"WARNING: {bq_type} íƒ€ì…ì€ BigQuery íŒŒí‹°ì…˜ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. íŒŒí‹°ì…˜ ì ˆì„ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                partition_clause = None
        
        # í´ëŸ¬ìŠ¤í„° ì ˆ ìƒì„±
        if cluster_columns:
            formatted_clusters = [self.format_identifier(col) for col in cluster_columns]
            cluster_clause = f"CLUSTER BY {', '.join(formatted_clusters)}"
        
        return partition_clause, cluster_clause
    
    def format_bigquery_type_with_precision(self, bq_type: str, oracle_type: str, 
                                          precision: Optional[str], scale: Optional[str], 
                                          char_length: Optional[str]) -> str:
        """BigQuery íƒ€ì…ì— ì •ë°€ë„/ìŠ¤ì¼€ì¼ ì •ë³´ ì¶”ê°€"""
        try:
            prec = int(precision) if precision and str(precision).strip() else None
            sc = int(scale) if scale and str(scale).strip() else None
            length = int(char_length) if char_length and str(char_length).strip() else None
        except (ValueError, TypeError):
            prec = None
            sc = None
            length = None
        
        # NUMERIC/BIGNUMERIC íƒ€ì…ì— ì •ë°€ë„ì™€ ìŠ¤ì¼€ì¼ ì¶”ê°€
        if bq_type in ['NUMERIC', 'BIGNUMERIC']:
            if prec is not None and sc is not None:
                # BigQuery NUMERIC ì œí•œì‚¬í•­ í™•ì¸
                if bq_type == 'NUMERIC':
                    # NUMERIC(P, 0)ì—ì„œ P > 29ì¸ ê²½ìš° BIGNUMERICìœ¼ë¡œ ë³€ê²½
                    if sc == 0 and prec > 29:
                        return f"BIGNUMERIC({prec}, {sc})"
                    # NUMERIC(P, S)ì—ì„œ P > 38 ë˜ëŠ” S > 9ì¸ ê²½ìš° BIGNUMERICìœ¼ë¡œ ë³€ê²½
                    elif prec > 38 or sc > 9:
                        return f"BIGNUMERIC({prec}, {sc})"
                return f"{bq_type}({prec}, {sc})"
            elif prec is not None:
                # ì •ë°€ë„ë§Œ ìˆëŠ” ê²½ìš°ë„ ë™ì¼í•œ ì œí•œì‚¬í•­ ì ìš©
                if bq_type == 'NUMERIC' and prec > 29:
                    return f"BIGNUMERIC({prec})"
                return f"{bq_type}({prec})"
        
        # STRING íƒ€ì…ì— ê¸¸ì´ ì •ë³´ ì¶”ê°€ (ì„ íƒì )
        if bq_type == 'STRING' and length is not None:
            if self.string_mode == 'string_only':
                # string_only ëª¨ë“œ: ê¸¸ì´ ì •ë³´ ë¬´ì‹œí•˜ê³  ë‹¨ìˆœ STRING
                return 'STRING'
            elif self.preserve_string_length:
                # auto ëª¨ë“œ + preserve_string_length: ê¸¸ì´ ì •ë³´ í¬í•¨
                return f"STRING({length})"
            else:
                # auto ëª¨ë“œ: ê¸¸ì´ ì •ë³´ ì—†ì´ STRING
                return 'STRING'
        
        return bq_type
    
    def create_column_description(self, col: Dict) -> Optional[str]:
        """ì»¬ëŸ¼ ì„¤ëª… ìƒì„± (Oracle ì½”ë©˜íŠ¸ë§Œ ë˜ëŠ” ê³µë€)"""
        # Oracle ì½”ë©˜íŠ¸ê°€ ìˆìœ¼ë©´ ê·¸ê²ƒë§Œ ì‚¬ìš©, ì—†ìœ¼ë©´ None ë°˜í™˜
        column_comment = col.get('column_comment', '').strip()
        if column_comment:
            return column_comment
        
        # ì½”ë©˜íŠ¸ê°€ ì—†ìœ¼ë©´ None ë°˜í™˜ (description ì—†ìŒ)
        return None
    
    def escape_description(self, description: str) -> str:
        """ì„¤ëª… í…ìŠ¤íŠ¸ë¥¼ SQLì—ì„œ ì•ˆì „í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì´ìŠ¤ì¼€ì´í”„"""
        if not description:
            return ""
        
        # ë”°ì˜´í‘œ ì´ìŠ¤ì¼€ì´í”„
        escaped = description.replace('"', '\\"')
        
        # ì¤„ë°”ê¿ˆ ë¬¸ì ì œê±°
        escaped = escaped.replace('\n', ' ').replace('\r', ' ')
        
        # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ ì¤„ì„
        import re
        escaped = re.sub(r'\s+', ' ', escaped)
        
        return escaped.strip()
    
    def show_version(self):
        """ë²„ì „ ì •ë³´ í‘œì‹œ"""
        print("Oracle to BigQuery Migration Tool - Portable Version")
        print("Version: 1.0.0")
        print("Python:", sys.version.split()[0])
        print("Platform: Portable (No pandas)")
    
    def show_help(self):
        """ë„ì›€ë§ í‘œì‹œ"""
        help_text = """
Oracle to BigQuery Migration Tool - Portable Version

ì‚¬ìš©ë²•:
  oracle-to-bq convert <input_file> [--output-dir <output_dir>] [ì˜µì…˜]
  oracle-to-bq init-config [config_file]
  oracle-to-bq --version
  oracle-to-bq --help
  oracle-to-bq --test

ëª…ë ¹ì–´:
  convert     Oracle ìŠ¤í‚¤ë§ˆ CSV íŒŒì¼ì„ BigQuery DDLë¡œ ë³€í™˜
  init-config ì„¤ì • íŒŒì¼ í…œí”Œë¦¿ ìƒì„±
  --version   ë²„ì „ ì •ë³´ í‘œì‹œ
  --help      ì´ ë„ì›€ë§ í‘œì‹œ
  --test      í¬í„°ë¸” íŒ¨í‚¤ì§€ í…ŒìŠ¤íŠ¸

ì˜µì…˜:
  --output-dir <output_dir>         ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: ì…ë ¥ íŒŒì¼ê³¼ ë™ì¼í•œ ìœ„ì¹˜)
  --project-id <project_id>         BigQuery í”„ë¡œì íŠ¸ ID
  --config <config_file>            ì„¤ì • íŒŒì¼ ê²½ë¡œ
  --string-mode auto|string_only    ë¬¸ìì—´ ë³€í™˜ ëª¨ë“œ
  --preserve-string-length          STRING íƒ€ì…ì— ê¸¸ì´ ì •ë³´ í¬í•¨
  --files                           ê°œë³„ íŒŒì¼ë¡œ DDL ìƒì„± (ê¸°ë³¸: ë³‘í•© íŒŒì¼)
  --no-primary-keys                 ê¸°ë³¸í‚¤ ì œì•½ì¡°ê±´ ìƒì„± ì•ˆí•¨
  --create-or-replace               CREATE OR REPLACE TABLE ì‚¬ìš©

ì˜ˆì‹œ:
  # ì„¤ì • íŒŒì¼ ìƒì„±
  oracle-to-bq init-config my_config.json
  
  # ê¸°ë³¸ ë³€í™˜ (ë³‘í•© íŒŒì¼ë¡œ ìƒì„±)
  oracle-to-bq convert schema.csv --output-dir bigquery_ddl --project-id my-project
  
  # ì„¤ì • íŒŒì¼ ì‚¬ìš©
  oracle-to-bq convert schema.csv --output-dir output --config my_config.json
  
  # ì˜µì…˜ ì‚¬ìš©
  oracle-to-bq convert schema.csv --output-dir output --project-id my-project --preserve-string-length
  
  # ê°œë³„ íŒŒì¼ë¡œ ìƒì„±
  oracle-to-bq convert schema.csv --output-dir output --project-id my-project --files

ì§€ì›í•˜ëŠ” ì…ë ¥ í˜•ì‹:
  í•„ìˆ˜: TABLE_NAME, COLUMN_NAME, DATA_TYPE, NULLABLE
  ì„ íƒ: OWNER (Oracle ìŠ¤í‚¤ë§ˆëª…, BigQuery ë°ì´í„°ì…‹ëª…ìœ¼ë¡œ ì‚¬ìš©)
        DATA_PRECISION, DATA_SCALE, DATA_LENGTH
        IS_PRIMARY_KEY, DATA_DEFAULT, COLUMN_COMMENT

"""
        print(help_text)

def show_help():
    """ë„ì›€ë§ í‘œì‹œ"""
    help_text = """
ğŸ”„ Oracle to BigQuery Migration Tool - Portable Version

ì‚¬ìš©ë²•:
  oracle-to-bq convert <input_file> [--output-dir <output_dir>] [ì˜µì…˜]
  oracle-to-bq init-config [config_file]
  oracle-to-bq --version
  oracle-to-bq --help
  oracle-to-bq --test

ëª…ë ¹ì–´:
  convert       Oracle ìŠ¤í‚¤ë§ˆ CSV íŒŒì¼ì„ BigQuery DDLë¡œ ë³€í™˜
  init-config   ì„¤ì • íŒŒì¼ í…œí”Œë¦¿ ìƒì„±
  --version     ë²„ì „ ì •ë³´ í‘œì‹œ
  --help        ì´ ë„ì›€ë§ í‘œì‹œ
  --test        í¬í„°ë¸” íŒ¨í‚¤ì§€ í…ŒìŠ¤íŠ¸

ì˜µì…˜:
  --output-dir <output_dir>         ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: ì…ë ¥ íŒŒì¼ê³¼ ë™ì¼í•œ ìœ„ì¹˜, íŒŒì¼ëª….sql)
  --project-id <project_id>         BigQuery í”„ë¡œì íŠ¸ ID (í•„ìˆ˜)
  --config <config_file>            ì„¤ì • íŒŒì¼ ê²½ë¡œ (JSON/YAML)
  --string-mode auto|string_only    ë¬¸ìì—´ ë³€í™˜ ëª¨ë“œ
                                    auto: ê¸°ë³¸ ë³€í™˜ (ê¸°ë³¸ê°’)
                                    string_only: ëª¨ë“  ë¬¸ìì—´ì„ STRINGìœ¼ë¡œ
  --preserve-string-length          STRING íƒ€ì…ì— ê¸¸ì´ ì •ë³´ í¬í•¨ (ì˜ˆ: STRING(100))
  --files                           ê°œë³„ íŒŒì¼ë¡œ DDL ìƒì„± (ê¸°ë³¸: ë³‘í•© íŒŒì¼)
  --no-primary-keys                 ê¸°ë³¸í‚¤ ì œì•½ì¡°ê±´ ìƒì„± ì•ˆí•¨
  --create-or-replace               CREATE OR REPLACE TABLE ì‚¬ìš©

ì˜ˆì‹œ:
  # ê¸°ë³¸ ë³€í™˜ (ì…ë ¥ íŒŒì¼ê³¼ ê°™ì€ ìœ„ì¹˜ì— schema.sql ìƒì„±)
  oracle-to-bq convert schema.csv --project-id my-project
  
  # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì§€ì •
  oracle-to-bq convert schema.csv --output-dir bigquery_ddl --project-id my-project
  
  # ì„¤ì • íŒŒì¼ ì‚¬ìš©
  oracle-to-bq convert schema.csv --output-dir output --config my_config.json
  
  # ì˜µì…˜ ì‚¬ìš©
  oracle-to-bq convert schema.csv --output-dir output --project-id my-project --preserve-string-length

ì§€ì›í•˜ëŠ” ì…ë ¥ í˜•ì‹:
  í•„ìˆ˜: TABLE_NAME, COLUMN_NAME, DATA_TYPE, NULLABLE
  ì„ íƒ: OWNER (Oracle ìŠ¤í‚¤ë§ˆëª…, BigQuery ë°ì´í„°ì…‹ëª…ìœ¼ë¡œ ì‚¬ìš©)
        DATA_PRECISION, DATA_SCALE, DATA_LENGTH
        IS_PRIMARY_KEY, DATA_DEFAULT, COLUMN_COMMENT

ì¶œë ¥:
  - Oracle OWNER â†’ BigQuery ë°ì´í„°ì…‹ëª… (ì†Œë¬¸ì ë³€í™˜)
  - Oracle ì½”ë©˜íŠ¸ê°€ ìˆìœ¼ë©´ descriptionìœ¼ë¡œ í¬í•¨, ì—†ìœ¼ë©´ description ì—†ìŒ
  - ì •ë°€ë„/ìŠ¤ì¼€ì¼ ì •ë³´ ì™„ë²½ ë³´ì¡´
  - í•œê¸€ í…Œì´ë¸”ëª…/ì»¬ëŸ¼ëª… ë°±í‹± ì²˜ë¦¬
"""
    print(help_text)

class OracleToBigQueryConverter:
    """Oracle to BigQuery ë³€í™˜ê¸°"""
    
    def __init__(self, project_id, string_mode='auto', preserve_string_length=False):
        self.project_id = project_id
        self.string_mode = string_mode
        self.preserve_string_length = preserve_string_length
    
    def test_package(self):
        """í¬í„°ë¸” íŒ¨í‚¤ì§€ í…ŒìŠ¤íŠ¸"""
        print("ğŸ§ª í¬í„°ë¸” íŒ¨í‚¤ì§€ í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        # ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
        test_data = [
            {'TABLE_NAME': 'TEST_TABLE', 'COLUMN_NAME': 'ID', 'DATA_TYPE': 'NUMBER', 'DATA_PRECISION': '10', 'DATA_SCALE': '0', 'NULLABLE': 'N'},
            {'TABLE_NAME': 'TEST_TABLE', 'COLUMN_NAME': 'NAME', 'DATA_TYPE': 'VARCHAR2', 'DATA_PRECISION': '', 'DATA_SCALE': '', 'NULLABLE': 'Y'}
        ]
        
        # ì„ì‹œ CSV íŒŒì¼ ìƒì„±
        temp_csv = Path("test_schema.csv")
        with open(temp_csv, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=['TABLE_NAME', 'COLUMN_NAME', 'DATA_TYPE', 'DATA_PRECISION', 'DATA_SCALE', 'NULLABLE'])
            writer.writeheader()
            writer.writerows(test_data)
        
        # ë³€í™˜ í…ŒìŠ¤íŠ¸
        temp_output = Path("test_output")
        success = self.process_csv_file(temp_csv, temp_output)
        
        # ì •ë¦¬
        if temp_csv.exists():
            temp_csv.unlink()
        if temp_output.exists():
            import shutil
            shutil.rmtree(temp_output)
        
        if success:
            print("âœ… í¬í„°ë¸” íŒ¨í‚¤ì§€ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
            return True
        else:
            print("âŒ í¬í„°ë¸” íŒ¨í‚¤ì§€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨!")
            return False


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    if len(sys.argv) < 2:
        tool = SimpleMigrationTool()
        tool.show_help()
        return
    
    command = sys.argv[1]
    
    if command == '--version':
        tool = SimpleMigrationTool()
        tool.show_version()
    elif command == '--help':
        tool = SimpleMigrationTool()
        tool.show_help()
    elif command == '--test':
        tool = SimpleMigrationTool()
        success = tool.test_package()
        sys.exit(0 if success else 1)
    elif command == 'init-config':
        # ì„¤ì • íŒŒì¼ ìƒì„±
        config_file = 'oracle_to_bq_config.json'
        if len(sys.argv) > 2:
            config_file = sys.argv[2]
        
        tool = SimpleMigrationTool()
        tool.create_config_template(config_file)
        sys.exit(0)
    elif command == 'convert':
        if len(sys.argv) < 3:
            print("âŒ ì‚¬ìš©ë²•: oracle-to-bq convert <input_file> [--output-dir <output_dir>] [ì˜µì…˜]")
            print("ì˜µì…˜:")
            print("  --project-id <project_id>         BigQuery í”„ë¡œì íŠ¸ ID")
            print("  --config <config_file>            ì„¤ì • íŒŒì¼ ê²½ë¡œ")
            print("  --string-mode auto|string_only    ë¬¸ìì—´ ë³€í™˜ ëª¨ë“œ (ê¸°ë³¸: auto)")
            print("  --preserve-string-length          STRING íƒ€ì…ì— ê¸¸ì´ ì •ë³´ í¬í•¨")
            print("  --files                           ê°œë³„ íŒŒì¼ë¡œ DDL ìƒì„± (ê¸°ë³¸: ë³‘í•© íŒŒì¼)")
            print("  --no-primary-keys                 ê¸°ë³¸í‚¤ ì œì•½ì¡°ê±´ ìƒì„± ì•ˆí•¨")
            print("  --create-or-replace               CREATE OR REPLACE TABLE ì‚¬ìš©")
            sys.exit(1)
        
        input_file = Path(sys.argv[2])
        
        # ì˜µì…˜ íŒŒì‹±
        output_dir = None  # ê¸°ë³¸ê°’ì€ None (ë‚˜ì¤‘ì— ì…ë ¥ íŒŒì¼ ê¸°ë°˜ìœ¼ë¡œ ì„¤ì •)
        string_mode = 'auto'
        preserve_string_length = False
        project_id = None
        config_file = None
        
        # --output-dir ì˜µì…˜ ì°¾ê¸°
        has_output_dir = False
        try:
            output_idx = sys.argv.index('--output-dir')
            if output_idx + 1 < len(sys.argv):
                output_dir = Path(sys.argv[output_idx + 1])
                has_output_dir = True
        except ValueError:
            pass
        
        # --project-id ì˜µì…˜ ì°¾ê¸°
        try:
            project_idx = sys.argv.index('--project-id')
            if project_idx + 1 < len(sys.argv):
                project_id = sys.argv[project_idx + 1]
        except ValueError:
            pass
        
        # --config ì˜µì…˜ ì°¾ê¸°
        try:
            config_idx = sys.argv.index('--config')
            if config_idx + 1 < len(sys.argv):
                config_file = sys.argv[config_idx + 1]
        except ValueError:
            pass
        
        # --string-mode ì˜µì…˜ ì°¾ê¸°
        try:
            string_mode_idx = sys.argv.index('--string-mode')
            if string_mode_idx + 1 < len(sys.argv):
                string_mode = sys.argv[string_mode_idx + 1]
                if string_mode not in ['auto', 'string_only']:
                    print("âŒ --string-modeëŠ” 'auto' ë˜ëŠ” 'string_only'ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
                    sys.exit(1)
        except ValueError:
            pass
        
        # --preserve-string-length ì˜µì…˜ í™•ì¸
        if '--preserve-string-length' in sys.argv:
            preserve_string_length = True
        
        # --files ì˜µì…˜ í™•ì¸ (ê°œë³„ íŒŒì¼ ìƒì„±)
        separate_files = '--files' in sys.argv
        
        # --no-primary-keys ì˜µì…˜ í™•ì¸
        create_primary_keys = '--no-primary-keys' not in sys.argv
        
        # --create-or-replace ì˜µì…˜ í™•ì¸
        create_or_replace = '--create-or-replace' in sys.argv
        
        # ë„êµ¬ ì´ˆê¸°í™”
        tool = SimpleMigrationTool(config_file=config_file)
        
        # ëª…ë ¹í–‰ ì˜µì…˜ìœ¼ë¡œ ì„¤ì • ë®ì–´ì“°ê¸°
        if project_id:
            tool.project_id = project_id
        if string_mode != 'auto':
            tool.string_mode = string_mode
        if preserve_string_length:
            tool.preserve_string_length = preserve_string_length
        if separate_files:
            tool.merge_output = False  # --files ì˜µì…˜ì´ ìˆìœ¼ë©´ ê°œë³„ íŒŒì¼ ìƒì„±
        if not create_primary_keys:
            tool.create_primary_keys = create_primary_keys
        if create_or_replace:
            tool.create_or_replace = create_or_replace
        
        if not input_file.exists():
            print(f"âŒ ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_file}")
            sys.exit(1)
        
        # output_dirì´ ì§€ì •ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì…ë ¥ íŒŒì¼ ê¸°ë°˜ìœ¼ë¡œ ì„¤ì •
        if output_dir is None:
            # ì…ë ¥ íŒŒì¼ê³¼ ê°™ì€ ë””ë ‰í† ë¦¬ì— íŒŒì¼ëª…ë§Œ .sqlë¡œ ë³€ê²½
            output_dir = input_file.parent
            # ë³‘í•© íŒŒì¼ëª…ì„ ì…ë ¥ íŒŒì¼ëª….sqlë¡œ ì„¤ì •í•˜ê¸° ìœ„í•´ toolì— ì „ë‹¬
            tool.output_filename = input_file.stem + '.sql'
        else:
            tool.output_filename = 'merged_ddl.sql'  # ê¸°ë³¸ ë³‘í•© íŒŒì¼ëª…
        
        success = tool.process_csv_file(input_file, output_dir)
        sys.exit(0 if success else 1)
    else:
        print(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹ì–´: {command}")
        tool = SimpleMigrationTool()
        tool.show_help()
        sys.exit(1)


if __name__ == "__main__":
    main()